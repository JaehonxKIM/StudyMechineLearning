# StudyMechineLearning

### 1일차
  - 머신러닝의 개요(구글 collab사용)

### 2일차 
  - 머신러닝을 활용한 데이터 분석 플로우

### 3일차
  ### 개요
    - 플라스크 기반 웹서비스
    - 머신러닝 모델을 적용한 서비스
    - 파파고 유사 형태 제공

  ### 사전지식
    - 웹 분야중 클라이언트 구성은 3개로 구성
        - html5 : 뼈대, 콘텐츠
        - css3  : 디자인, 레이아웃, 데코레이터, 반응형
        - javascript : 이벤트, ajax(비동기 네트워크), 화면조작(동적 화면 구성), 이 파트는 써드파트 모듈이 다양하게 존재함(jQuery, bootstrap,..)
    - 클라이언트가 서버로 데이터를 전송하는 방법
        - form 전송 : 주소창 옆에 새로고침 아이콘이 x로
            변경된다(아주 짧게). 그리고 화면이 새로고침 된다
        - ajax 전송 : 화면 변화 없다. 응답 결과로 변경할수는 있지만 페이지는 고정되어 있다
            - 파파고와 유사해야 하므로, 이 방식으로 진행

  ### 프로젝트 구조
    /
    L templates  : *.html이 위치하고, 화면 랜더링시 사용
        L index.html
    L run.py     : 엔트리 포인트
    
    L readme.md  : 코멘트, 설명

### 4일차
  - 모듈 삽입

### 5일차
  - 성능 평가
    - iris 데이터를 활용해서 어느 알고리즘이 높은 정확도를 보여주는지 평가
  - 교차 검증
    - breast_cancer 데이터를 활용해서 독립변수와 종속변수의 shape을 알아보고 학습시키고 성능평가
    - 파이프라인 구축 과정 초반까지

### 6일차 
  - 하이퍼파라미터 튜닝
  - 최적의 파라미터 값 구하기
  - 파이프라인 구성 
  - 의사결정트리의 개요
    - 결정트리 의사결정 분기과정 시각화
    - 과적합 처리
    - 등고선 plot 그리기 기본

### 7일차 
  - 과적합 처리
  - 앙상블 
    - 랜덤포레스트
  - 보팅
    - 소프트보팅 : 다 더해서 평균
    - 하드보팅 : 다수결
  - 부트 스트래핑
    - 성능평가
    - 시각화
  - 부스팅
    - AdaBoost
    - GBM
    - XGBoost
  - 도구 관련 파라미터
    - 평가도구
      - rmse(root mean square error) : 정답과 예측 사이의 오차율 -> 작을수록 정확도가 높다
      - mse(mean square error) : 평균 제곱근 오차
      - mae(mean absolute error) : 평균 절대값 오차 -> 분류쪽에 활용
      - logloss : 손실에 로그처리
      - error
      - merror
      - mlogloss
      - auc
  - 머신러닝 과정(sklearn) -> 사이킷런 스타일로 Wrapping된 클래스 생성 -> fit() -> predict() -> 평가도구를 활용하여 판단

### 8일차
  - 스태킹(Stacking) : 
    - 원리
      - 여러 모델의 예측결과를 기반으로, 이 정보를 메타정보로 구축, 이를 이용해서 다시 학습 후 예측 수행
      - 모델은 2단계에 걸쳐서 생성
        - 예측결과가 데이터가 되는 구조
    - 정리
      - 1차 모델: 원본 데이터를 이용하여, 여러개의 알고리즘을 활용(서로다른)
      - 2차 모델 : 예측 결과를 데이터로 활용하여서 학습후 예측 
  - SVM
    - 가장 넢은 여백을 가지게끔, 클래스로 구분할 수 있게 경계선을 찾는다
    - 2개의 클래스를 확실하게 분류할 수 없다면, 가능한 최적의 경계선을 찾는다
    - 이미지 분류에서 많이 사용(전통적)
  - KNN 
    - 클러스터링에서 많이 사용  
  - CLUSTERING
    - 알고리즘을 활용하여 무리(군집)를 만들어내어 새로운 특징점을 혹은 분류라는 지점을 생성하는 기법

### 9일차
  - 군집화(Clustering)
  - K-MEAN(거리기반)
    - 1. 최초 알고리즘 생성시, 군집 중심점을 총 몇개 만들지 결정(제시, 기본값활용, 추천받을 수도 있다), 아래 그림에서는 2개로 설정
    - 2. 중심점 설정
        - 랜덤, 운이 나쁘면 오랜 시간이 걸릴 수도 있다
        - 데이터가 많이 밀집한 위치에 평균적으로 중심점을 잡으면, 상대적으로 빠르게 최종 중심점을 찾을 수 있다
    - 3. 중심점에 가까운 데이터 포인트들은 해당 중심점에 멤버로 소속된다 -> 거리계산
    - 4. 군집을 중심으로 모든 멤버들간의 평균 지점으로 중심점을 이동시킨다
    - 5. 반복(3~4번 절차 반복)
    - 6. 중심점이 몇번까지 이동할 것인지 제약, 더 이상 중심점이 변경되지 않으면 군집화 종료된다
    - 장점
      - 매커니즘이 쉽다 -> 가장 많이 활용
      - 알고리즘도 심플
    - 단점
      - 데이터가 많다면, 군집의 정확도가 떨어질 수 있다(평가를 통해 확인)
      - PCA등을 이용하여 압축처리가 필요
      - 3~4단계 지속적으로 발생되면 군집화 시간이 증가된다
      - 중심점을 몇개 잡을 지 애매하다
      - 후보군을 몇개 잡고 군집화 후 평가하여 결정
  - 차원축소 사용(PCA)
    - 차원축소가 효율적으로 작동하려면, 분류에 영향을 미치지 않은 중요하지 않은 컬럼들 제거 후 진행하는 것이 효율적
    - 차원축소가 군집의 성능을 높이지는 않는다(보장하지 않는다)
  - 군집평가
  - 실루엣 분석
    - 실루엣 계수 : 개별 데이터가 가지는 군집화 지표
    - 1 ~ 1 값이 나온다
      - 1에 가까울수록 두개의 군집화가 잘 되었다
      - 0에 가까울수록 재작업의 필요성이 보인다
      - 1에 가까울수록 다시 작업해야 한다
      - 전체 실루엣 계수 평균값과 개별 군집의 평균값 차이가 적을수록 좋다라고 판단(편차가 적을수록)
  - 밀도기반(MEAN-SHIFT) : 특정 대역폭(bandwidth)을 이용하여 최조의 확률 밀도 중심이 높은 쪽으로 중심점을 이동하는 방식
    - 장점
      - 군집화가 유연하게 구성된다
      - 군집개수를 미리 정할 필요 없다
      - 이상치 데이터가 군집에 영향을 미치지 않는다
    - 단점
      - 데이터가 많으면 많을수록 시간이 많이 소요된다
      - 대역폭에 따른 성능차이가 크다
    - 사용처 : 이미지, 영상쪽에 많이 활용된다


