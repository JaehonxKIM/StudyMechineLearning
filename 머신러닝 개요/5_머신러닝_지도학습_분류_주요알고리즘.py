# -*- coding: utf-8 -*-
"""5.머신러닝_지도학습_분류_주요알고리즘.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pmUJZG8wYkzwcI0JFOKjWQTlpwv_swmW

# 결정트리

## 개념

- 데이터에 들어있는 규칙을 학습을 통해 찾고, **트리 기반**으로 **분류의 규칙을 만들어 내는 알고리즘이다**
- 성능
  - 어떤 기준의 규칙인가(규칙을 만드는 룰, 임계값)
  - 분류를 하는 규칙 어떻게 효율적으로 배치하는가
  - **깊이**
"""

from google.colab import drive
drive.mount('/content/drive')

from IPython.display import Image
Image('/content/drive/MyDrive/ComputerProgramming/today/분류_결정트리.jpg')

"""```
- 전체 데이터를 특정 노드에서 분류가 되어 전체적으로 지도학습의 분류 주제를 구현하는 알고리즘
- 노드
  - 노드 : 분기점, 가지의 맨끝점, 시작점
  - 루트노드 : 데이터의 분기되는 시작점
  - 규칙노드 : 규칙(룰)이 존재, 룰에 의해서 데이터를 분기한다.피쳐(특성, 독립변수)들을 결합, 조합하여 규칙을 생성하고, 그 기준으로 분기를 하는 노드, 결과물은 브랜치가 되어서 서브트리가 만들어진다
  - 리프노드 : 더 이상의 분기점이 없다(끝) -> 어떤 결정된 클래스(종속변수)값을 가진다
- 브랜치/서브트리 : 새로운 규칙이 조건에 의해서 생성되면, 서브트리가 생겼다 표현한다

- depth(깊이) : 깊을수록, 정확도는 올라가겠지만, 성능저하의 원인, 과적합의 원인 -> 적절한 깊이감에 대한 조절 -> 이를 처리하기 위한 임계값 필요
- 목표 포인트
  - 데이터가 균일하게 분류되도록 분할 처리하는 것
  - 데이터의 균일성을 스케일을 통해서 처리하는 것
  - 과적합에 대한 대비, 깊이감 조절, 데이터의 쏠림을 방지
```

"""

from IPython.display import Image
Image('/content/drive/MyDrive/ComputerProgramming/0404_res/ml-의사결정트리.png')

"""## 적용"""

# 1. 모듈 가져오기
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

"""## 하이퍼 파라미터

- criterion
  - 개요
    - **결정트리 알고리즘**에서 **데이터의 균일성**(정보의 균일성)이 높게 되도록 데이터 세트가 선택되게 **규칙을 만드는 것이 중요**하다

  - 지표(방법)
    - "entropy"
      - 엔트로피를 이용한 정보이득지수(information gain)
      - 혼잡도
      - 엔트로피는 주어진 데이터 집합의 혼잡도를 표현
      - 서로 다른 데이터들이 뒤섞여 있다면 엔트로피가 높다
      - 같은 데이터들이 모여 있으면 엔트로피가 낮다
      - 정보이득지수 = 1 - 엔트로피지수값
        - **판단 : 정보이득지수가 임계값(설정값)보다 크다면 규칙노드로 판단이 된다 -> 브랜치(가지치기) 실행**

    - "gini"
      - 불평등 지수(경제학에서표현)에서 나오는 지표
      - 0: 가장 평등
      - 1: 1에 가까울수록 불평등해진다
      - 데이터(분류된 데이터 기준)가 다양하면 평등, 특정값에 쏠리면 불평등
"""

Image('/content/drive/MyDrive/ComputerProgramming/today/분류_결정트리2.jpg')
# 엔트로피 관점에서 해석
  # A SET : 여러정보가 섞여있다 -> 혼탁
  # C SET : 정보가 균일하다 -> 특정 데이터로 쏠려있다
# 지니 계수
  # A SET : 여러 데이터가 섞여있다 -> 평등하다
  # C SET : 특정 데이터가 균일하다(특정 데이터로 쏠려있다) -> 불평등하다 => 1로 수렴

# 2. 알고리즘 생성
'''
criterion : 규칙 노드는 만드는 판단기준 엔트로피 혹은 지니계수 사용하는 기준만 지정, 임계값 접근은 
            파라미터에서 미지원 -> 튜닝 포인트는 엔트로피 혹은 지니 둘중 어느쪽에서 더 성능이 
            나오는지 확인
'''
clf = DecisionTreeClassifier( )

Image('/content/drive/MyDrive/ComputerProgramming/today/분류_결정트리3.jpg')
# 가지치기 원리
# 1번은 노드에 도착했고, 데이터를 살펴보니 모두 같은 것만 남았다 -> 리프노드가 된다(끝)
# 1 -> 2번은 더이상 분류할게 없다 : 리프노드가 되었다
# 1 -> 2-2번은 규칙노드를 만들어야 한다 -> 기준이 필요(지니, 엔트로피 지정)
#   기준 만족하면 브랜치 노드 작동(가지치기 작동)
# 4번은 기준이 만족할 때까지 반복

"""## 과적합 방지 하이퍼파라미터

- 깊이를 어느 정도 줄것인가?
- 리프노드를 몇개로 갈것인가?
- 규칙노드로 가지치키를 할대 전제 조건을 어떻게 할것인가?
"""

Image('/content/drive/MyDrive/ComputerProgramming/today/분류_결정트리4.jpg')
# min_samples_split
# 규칙노드가 만들어지는 기본 전제 조건으로 몇개의 샘플데이터가 남았을 때 스플릿을 할 것인가
# 값이 작을 수록 튀는 값도 다 잡아낼 수 있고 -> 과적합이 될 것 -> 아주 작게 1을 지양하지 말자
# min_samples_leaf
# 리프노드가 되기위해 해당 노드에서 최소 몇개의 샘플을 가지고 있는가? 설정값
# 최소 이정도 샘플은 들고 있어야 리프 노드가 된다

Image('/content/drive/MyDrive/ComputerProgramming/today/분류_결정트리5.jpg')
# 최대 특정 수
# 최대 깊이
# 최대 리프노드 수

# 2. 알고리즘 생성
'''
criterion : 규칙 노드는 만드는 판단기준 엔트로피 혹은 지니계수 사용하는 기준만 지정, 임계값 접근은 
            파라미터에서 미지원 -> 튜닝 포인트는 엔트로피 혹은 지니 둘중 어느쪽에서 더 성능이 
            나오는지 확인
min_samples_split
min_samples_leaf
max_features
max_depth
max_leaf_node
random_state = 알고리즘 생성시 사용하는, 내부에서 사용하는 모든 난수의 씨드 설정
- 고정하면 언제나 동일한 성능을 내는 알고리즘 생성
'''
clf = DecisionTreeClassifier(random_state= 0)
clf

# 3. 데이터 준비
tmp = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    tmp.data, tmp.target, test_size = 0.25, random_state = 0)
X_train.shape, X_test.shape,

# 4. 훈련
clf.fit( X_train, y_train )

# 5. 예측 및 성능평가
clf.score( X_test, y_test )

"""## 결정트리 분기과정
- 결정 트리가 어떤 과정을 거쳐서, 노드를 분류, 생성했는지 시각적으로 확인
"""

from sklearn.tree import export_graphviz
# 모델을 덤프해서 데이터 정답을 같이 표기하여 덤프
export_graphviz(clf, out_file = 'dtree.model',     # 학습된 알고리즘, 파일명
                feature_names= tmp.feature_names,  # 특성 이름
                class_names = tmp.target_names,    # 정답 이름
                filled = True)

# 시각화
import graphviz
# graphviz 라이브러리 활용해서 덤프파일(모델, 정답, 정답의 이름)을 읽어들여서 시각화를 할 수 있다
with open('dtree.model') as f:
  # 텍스트 형태로 읽었다
  mGraph = f.read()

# 시각화
graphviz.Source( mGraph )

"""- 결정트리 상에 규칙노드들을 체크해 보니, 사용하지 않은 특성(피쳐)가 보인다. 사용비중이 차이가 나는 것이 보인다.
- 결정트리의 분류를 완성할때 피쳐별 기여도가 다르다는 것을 의미
- 시각화를 해서 중요 피쳐를 표현
"""

import seaborn as sns
import numpy as np

clf.feature_importances_

f1 = np.round( clf.feature_importances_, 3 )
f1

sns.barplot(x = f1, y = tmp.feature_names)
# sepal length는 분류하는데 전혀 기여도가 없다
# 이 컬럼을 제외해도 결정 트리 알고리즘에서는 문제가 없다

"""## 과적합 처리
- 데이터를 생성해서 사용(범위 데이터로써, 특정 목적을 가지게 생성)
- make_xxxx()
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt

# %matplotlib inline

# 더미 데이터 생성
'''
n_samples : 생성되는 전체 데이터 수
n_features : 특성의 총 수, 독립변수의 총개수
n_informative : 독립변수들 중에서 종속변수(정답)와 상관관계가 있는 성분의 수
n_redundant : 독립변수들 중에서 다른 독립변수들과 선형 조합을 나타낼 수 있는 수  
n_classes : 종속변수의 개수 (정답의 수)
n_clusters_per_class : 클래스(정답) 당 클러스터(군집)의 개수
random_state : 데이터 생성시 난수를 사용하며느 그 씨드값을 제시, 미사용 -> 현재시간
'''
X, y = make_classification(n_features= 2, n_redundant = 0, n_informative = 2,
                    n_classes = 3, n_clusters_per_class = 1, random_state = 0)
X.shape, y.shape

X[0]

X[:, 0]

# scatter
# X축 첫번째 값 집합, y축에는 두번째 값 집합 사용
plt.scatter(X[:,0], X[:,1],
            s = 25,
            marker ='o',
            c = y,
            cmap = 'rainbow',
            edgecolors = 'k')
# 결정트리 관점에서 보면
# 청록색 데이터는 바로 분류가 가능하다
# 빨간색, 보라색은 몇번의 규칙노드를 통해서 영역을 쪼갤 수 있다

"""- 등고선 plot을 이용해서 분포싀 경계를 그릴수 있다
- 어떤  분류 모델이던, 모델, X,y를 넣으면 시각화 할 수 있게 제작
- 최종적으로는 2D로 표현
"""

# numpy로 시작값에서 끝값까지, 특정 값 간격으로, 구간 나누기
import numpy as np

# 1 ~ 2 사이를 4개의 포인트로 나눠주세요
x1 = np.linspace(1, 2, num = 4)
y1 = np.linspace(4, 5, num = 4)
x1, y1

# 위의 재료를 가지고 (특성1, 특성2)로 모인 데이터 형태로 가공하는게 목표
xx, yy = np.meshgrid(x1, y1)
xx

yy

xx.shape, yy.shape, xx.ndim, xx.dtype

# xx를 1차 배열로 바꾼다 -> flattern or reshape
print(xx.ravel())
print(xx.shape)
print(xx.ndim)

# x축에 대한 y축의 세트 구성 => (특성1, 특성2)
'''
[
  [특성1, 특성2],
  [xx의 첫번째, yy의 첫번째],
  ..
]
'''
np.c_[xx.ravel(), yy.ravel()]

def show_rect_clf_boundary( clf, X, y ):
  '''
    모델을, 이 안에서 학습하여서 그리겟다
    - clf : 분류기, 알고리즘
    - X   : feature data, 특성, 독립변수
    - y   : target data, 정답, 종속변수
  '''
  # 0. 차트 외관에 필요한 요소 준비
  _, ax = plt.subplots()
  # 1. 산포도(산점도) 드로잉
  ax.scatter( X[ :, 0], X[ :, 1], s=25, marker='o', c=y, cmap='rainbow', edgecolors='k',
              # 칼러 리미트 설정, 그리는 내용이 위로, 아래로 조정하는 zorder : depth 조정
             clim=( y.min(), y.max() ), zorder=3
             )
  # 2. 차트 외곽 정리
  ax.axis('off')   # 축 정보 삭제
  ax.axis('tight') # 데이터의 분포가 펼쳐졌을 때 빡빡한 형태를 띈다면 여유있게 조절
  
  # 3. 면적(같은 답을 가진 사각 영역에 대한 공간)에 대한 데이터 준비
  #    분류에 대한 경계성 계산 재료 준비
  #    화면 공간상의 픽셀을 구한다(그 픽셀의 좌표는 독립변수 2개가 된다)
  xlim_start, xlim_end = ax.get_xlim()
  ylim_start, ylim_end = ax.get_ylim()
  # 좌표계의 각 축별 시작값, 끝값
  #print(xlim_start, xlim_end, ylim_start, ylim_end)
  # 구간 간격을 200으로 설정
  x_std = np.linspace(xlim_start, xlim_end, num = 200)
  y_std = np.linspace(ylim_start, ylim_end, num = 200) 
  xx, yy = np.meshgrid(x_std, y_std)
  tmp_xy = np.c_[xx.ravel(), yy.ravel()]
  print(tmp_xy.shape)

  '''
    tmp_xy[ : , 0] : 준비한 좌표계 40000개중 x축 성분
    tmp_xy[ : , 1] : 준비한 좌표계 40000개중 y축 성분
    s: 점의 크기
    zorder : 그려지는 깊이, 낮을수록 밑으로, 클수록 위로
  '''
  ax.scatter(tmp_xy[:, 0],tmp_xy[:, 1], s=1, zorder=1)
  
  # 4. 알고리즘 생성(이미 완료됨)  
  # 5. 학습, 예측
  clf.fit(X,y)
  #Z = clf.predict()

  # 6. 등고선 플롯을 그리기 위한 작업(분류된 영역 그리기)
  pass

show_rect_clf_boundary(DecisionTreeClassifier(), X, y)

## 위의 코드와 같음(정리본)

def show_rect_clf_boundary( clf, X, y ):
  '''
    모델을, 이 안에서 학습하여서 그리겟다
    - clf : 분류기, 알고리즘
    - X   : feature data, 특성, 독립변수
    - y   : target data, 정답, 종속변수
  '''
  _, ax = plt.subplots()
  ax.scatter( X[ :, 0], X[ :, 1], s=25, marker='o', c=y, cmap='rainbow', edgecolors='k',clim=( y.min(), y.max() ), zorder=3)
  ax.axis('off')  
  ax.axis('tight') 
  xlim_start, xlim_end = ax.get_xlim()
  ylim_start, ylim_end = ax.get_ylim()
  x_std = np.linspace(xlim_start, xlim_end, num = 200)
  y_std = np.linspace(ylim_start, ylim_end, num = 200) 
  xx, yy = np.meshgrid(x_std, y_std)
  tmp_xy = np.c_[xx.ravel(), yy.ravel()]
   
  # 4. 학습, 예측
  clf.fit(X,y)
  # 5. 좌표계(혹은, 면적을 그리기 웨해 준비한 200*200 좌표 셋 데이터)에 대해 예측 수행
  Z = clf.predict(tmp_xy)
  print('좌표별로 예측된 분류값', Z, np.unique(Z))
  print(xx.shape, yy.shape, Z.shape, Z.reshape(xx.shape).shape)
  # 6. 등고선 플롯을 그리기 위한 작업(분류된 영역 그리기)

  ax.contourf( xx,
               yy,
               Z.reshape( xx.shape ),
               # 데코레이션 파트
               alpha  = 1.0,
               levels = np.arange( len( np.unique( Z ) ) + 1 ) - 0.5, # 면적 색상에 대한 준비
               cmap   = 'rainbow',
               clim   = ( y.min(), y.max()),
               zorder = 1
  )
  pass

show_rect_clf_boundary(DecisionTreeClassifier(), X, y)
# 과적합된 모델의 분류 지도 -> 모든 케이스를 거의다 분류했다 -> 과적합, 편향적이다
# 이를 해결하기 위해서는 알고리즘 레벨에서 과적합 방지 장치를 작동시켜야 한다

# 과적합 방지 파라미터를 리프노드의 최소 데이터 수를 조정해서
# 적당하게 분류지도가 완성되었다
# 실제 진행시에는 고려사항 및 테스트가 필요하다
show_rect_clf_boundary( DecisionTreeClassifier(
    min_samples_leaf = 6    
), X, y)

"""# 앙상블

## 개념

- **Ensemble Learning(앙상블 학습법)**
- **여러개의 알고리즘**(분류)을 생성(동일, 서로다른)하여 학습을 수행하고, **그 예측값을 통합**(평균, ...)해서 최종예측을 수행하는 기법
  - 약한 알고리즘들을 뭉쳐서 강한 모델을 만들 수 있다
- **단일 분류기(모델을 1개 사용)보다 높은** 성능 혹은 신뢰성을 얻을 수 있었다
"""

Image('/content/drive/MyDrive/ComputerProgramming/today/분류_앙상블1.jpg')
# 동일 알고리즘을 여러개 사용한 것인가? -> 배깅
# 다른 알고리즘을 여러개 사용한 것인가? -> 보팅

"""## 보팅

### 특징, 포인트

- Voting
- 서로 다른 알고리즘 사용
- 종류
  - **하드 보팅**
    - **다수결의 원칙**
  - **소프트 보팅**
    - 하드 보팅의 문제점 -> 다수결의 의존하다 보니, 질적인 부분, 실제 비중은 고려하지 못했다
    - **정답이 결정되는 모든 확률을 더해서 평균내서 높은 쪽으로 결론을 내는 방식을 취한다**
"""

Image('/content/drive/MyDrive/ComputerProgramming/today/분류_앙상블2_보팅.jpg')

"""### 보팅 구현, 실습

#### 필요 모듈 가져오기
"""

# 보팅 구현, 실습 -> 여러 알고리즘, 하드 혹은 소프트 보팅 방식 적용
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

# 보팅
from sklearn.ensemble import VotingClassifier

# 데이터셋
from sklearn.datasets import load_breast_cancer

# 데이터셋 준비(훈련용, 테스트용)
from sklearn.model_selection import train_test_split

# 평가도구
from sklearn.metrics import accuracy_score

# 데이터 처리
import pandas as pd

"""#### 데이터 준비"""

# 데이터 로드
cancer = load_breast_cancer()
# data_df를 이름으로 가진 DataFrame 구성
data_df = pd.DataFrame(cancer.data, columns = cancer.feature_names)
data_df.head(1)

data_df.shape

# 데이터 => 훈련용, 테스트용으로 분리
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,
                                                    test_size = 0.25, random_state = 0)
X_train.shape, X_test.shape

"""#### 보팅에 사용할 알고리즘 생성"""

lr_clf  = LogisticRegression()
knn_clf = KNeighborsClassifier(n_neighbors=8)

# 보팅에 사용할 알고리즘 연결
# soft 방식인 경우 알고리즘의 개수가 짝수개여도 동수가 나올 확률은 아주 적다
vo_clf = VotingClassifier(estimators = [('LR', lr_clf),('KNN',knn_clf)], voting = 'soft')

# 학습
vo_clf.fit( X_train, y_train )

# 예측
y_pred = vo_clf.predict(X_test)
y_pred

# 성능평가
accuracy_score(y_test, y_pred)

# 보팅에 참여한 개별 알고리즘은 어떻게 결과가 나오는지 확인
for clf in [LogisticRegression(), KNeighborsClassifier(n_neighbors = 8)] :
  clf.fit(X_train, y_train)
  y_pred = clf.predict(X_test)
  print(clf.__class__.__name__, accuracy_score( y_test, y_pred ))

# 현재 보팅에 참여한 멤버들 중 로지스틱 회귀가 성능이 더 높아서
# 모든 예측결과에 영향을 미치는 것으로 예상됨  
# 항상 그러지는 않다
# 발전사향 : 알고리즘 중성, 개별 알고리즘 최적화 => 보팅

"""### 앙상블 학습의 핵심포인트

- 트레이드-오프 문제
  - A가 높으면 B가 낮고, 상반성을 가진 개념들 사이의 문제
  - 상반된 2개의 개념을 적절하게 조정해야 트레이드-오프문제를 해결할 수 있다
  - 앙상블의 트레이드 오프
    - 편향과 분산
    - 편향 : 바이어스, 해당 값이 높으면 특정문제는 잘 맞춘다. 반면에 특정문제는 잘 못 맞추는 문제를 가지고 있다. 과소적합이 문제를 야기할 수 있다
    - 분산: 분산이 높으면, 이상치를 잘 뽑아낸다. 과대적합의 문제를 야기할 수 있다
    - 이 2개 개념을 잘 조절할수 있게 모델을 최적화 하는게 목표

### 부트 스크래핑

- cv(학습 데이터셋을 훈련용과 검증용으로 나눠서 처리하는 방식)는 앙상블 학습에서 어떻게 적용되는가
"""

Image('/content/drive/MyDrive/ComputerProgramming/today/분류_앙상블2_배깅_부트스트래핑.jpg')
# 서브셋에 중복된 데이터들이 여러번 등장한다 -> 기존 cv와는 다른점
# 앙상블 학습의 특이점

"""## 배깅-랜덤포레스트

- Bagging
- **동일 알고리즘을 여러개** 만들어서, 보팅을 수행한다
- 대표 알고리즘
  - 랜덤 포레스트
    - 뛰어난 성능, 빠른 수행시간, 유연성등등 
    - 기초 알고리즘으로 **결정트리를 사용**
"""

Image('/content/drive/MyDrive/ComputerProgramming/today/분류_앙상블2_배깅_랜덤포레스트.jpg')
# 아래그림
# 알고리즘 3개 사용
# 데이터셋은 중복된 데이터가 들어갈 수 있다 -> 부트 스트래핑
# 3개의 동일 알고리즘은 각각 다른 하이퍼파라미터를 사용하여 서로 다른 데이터를 가지고 학습/예측 수행
# 개별 확률을 더해서 평균을 내어 분류를 수행한다 -> 소프트 보팅 수행
# 이러 개념을 모두 포괄한 알고리즘이 랜덤 포레스트

"""- 랜덤 포레스트
  - **집단 학습 기반**
  - 구성에 따라 고밀도 정밀 분류, 회귀, 클러스터링도 모두 작업 가능하다
    - 하드/소프트 보팅 가능
    - 학습 데이터를 무작위로 샘플링을 수행한다 -> 랜덤
"""

Image('/content/drive/MyDrive/ComputerProgramming/0404_res/ml-랜덤포레스트.png')

# 위에서 사용한 동일한 암 데이터를 활용
X_train.shape, X_test.shape, y_train.shape, y_test.shape

# 알고리즘 생성
from sklearn.ensemble import RandomForestClassifier

'''
  하이퍼 파라미터
  n_estimators: 기본값 : 100, 결정트리 알고리즘을 몇개 사용할 것인가?
  결정트리의 파라미터이다 -> 동일한 파라미터를 가진 알고리즘은 다양한 데이터를 통해서 학습
  => 알고리즘별로 다른 파라미터를 넣는다면 더 좋을 수도 있지 않을까?
    => 동일 알고리즘이라도 하이퍼파라미터를 다르게 세팅하고 싶다 -> 하이퍼파라미터 튜닝 기법 적용 
'''
rf_clf = RandomForestClassifier(random_state = 0)

# 훈련
rf_clf.fit(X_train, y_train)

# 예측
rf_clf.predict(X_test)

# 평가
accuracy_score(y_test, y_pred)

"""- 정확도 상승 밑 알고리즘 파라미터 튜닝"""

from sklearn.model_selection import GridSearchCV

param_grid = {
  # '파라미터명' : [값의 후보들 ]
  'n_estimators' : [ 100, 200 ],       # 결정트리 개수. 사용하는 알고리즘 개수
  'max_depth'    : [ 6, 8, 10, 12 ],   # 결정트리의 최대 깊이값  배분
  'min_samples_split': [ 8, 12, 16 ],  # 규칙노드에서 브랜치를 수행하기 위한 최소 데이터셋 수
  'min_samples_leaf': [ 8, 16, 24]     # 리프노드로 결정되는 최소 데이터수 
}
# n_job : CPU 코어를 학습시 몇개를 사용할 것인가? -1 = 모든 코어 동원해서 학습
rf_clf = RandomForestClassifier(random_state=0, n_jobs = -1)

grid_cv = GridSearchCV( rf_clf, param_grid, cv = 5, n_jobs = -1)

# 학습
grid_cv.fit(X_train, y_train)

# 최적의 파라미터, 점수
# 랜덤포레스트의 내부에서 사용하는 결정트리의 하이퍼파라미터는 1개의 최저 세트로 사용된다
print(grid_cv.best_params_)
print(grid_cv.best_score_)

# 피쳐의 중요도
rf_clf = RandomForestClassifier(random_state = 0)
rf_clf.fit(X_train, y_train)

# 분류의 성능을 높이기 위해서 영향력이 적은 컬럼은 제거, 압축한다 => 그 판단을 위해서 시각화
rf_clf.feature_importances_

import matplotlib.pyplot as plt
import seaborn as sns

tmp = pd.Series(rf_clf.feature_importances_, index = cancer.feature_names)
tmp = tmp.sort_values(ascending= False)
tmp

# sns.barplot(x = rf_clf.feature_importances_, y = cancer.feature_names)
sns.barplot(x = tmp, y = tmp.index)
plt.show()
# 분류 모델이 분류를 잘하는데 영향을 미치는 컬럼의 지배력을 서열화해서 시각화

"""## 부스팅

- Boosting
- 최근 사용빈도가 높아진 알고리즘 형태
- 원리
  - 여러개의 약한 분류기(알고리즘)을 순차적으로 학습 예측하여서, **잘못 예측한 데이터에 가중치를 부여하여 오류를 개선하는 방식으로 학습을 진행**하는 방식
- 여러개의 알고리즘들은 각자의 방식으로 오류를 개선한다

### AdaBoost
"""

'''
- 데이터가 적당히 분포되어 있다(피쳐데이터셋 그림)
- step1 : 분류기준 1을 기준으로 +를 분류한다, +를 포함한다면 동그라미, 아닌 것은 잘못된 것으로 간주한다
          분류기준 1을 기반으로 왼쪽은 + 영역, 오른쪽은 - 영역이라고 한다면
          오른쪽에서 +를 포함하고 있다면 동그라미, 아닌 것은 그냥 둔다

- step2 : 잘못 분류된 데이터 =? 동그라미+ => 오류 => 오류에 가중치를 부여하여
          +가 두껍게 표시되었다

- step3 : 가중치를 받은 +를 포함하여 다시 분류 => + 영역이 넓어짐
          => -동그라미 등장

- step4 : 오류 데이터 => -동그라미 => 가중치 부여 => 두꺼운 -

- step5 : 반복 => 최종적으로 결합을 해보면 분류 기준이 생성된다 => 오랜 시간이 걸림          
'''
Image('/content/drive/MyDrive/ComputerProgramming/today/분류_앙상블3_부스트.jpg')

Image('/content/drive/MyDrive/ComputerProgramming/today/분류_앙상블3_부스트2.jpg')
# 여러번의 분류를 시도해서 그 가중치를 더해주면, 최종적으로 분류된 가중치 값들이 표현
# 진한 곳, 연한 곳, 중간값 등등 보인다

"""### GBM

- Gradient Boost Mechine
- 경사하강법을 가중치를 계산하는데 활용하는 기법
- 경사하강볍
  - 특정값(예 0.001)을 기반으로 지속적으로 업데이트(값을 미세조정)하면서, 예측값이 향상되었는지 체크 -> 시간이 지날수록 기울어진다(값의 추이)
  - 딥러닝이 최적화 기법에서 등장, 여러가지 기법들이 존재
  - 언제까지 미세조정을 할 것인가? 최적의 정확도가 나올때까지 반복 -> **학습시간이 많이 소요된다**

- AdaBoost 와의 차이점은 가중치 업데이트라는 차이
- 예
  - h(x) = y - F(x) = 실제값 - 예측값 = 오차값
  - 오차값을 최소화하는 방향으로 F(x)를 구성하는 특정 성분 중 가중치(W:weigt)를 미세조정한다
  - 오차값이 0에 수렴하도록 W를 조정한다
"""

from sklearn.ensemble import GradientBoostingClassifier

# 시간측정용
import time

# 시작 시간
start_time = time.time()

# GBM
gbm_clf = GradientBoostingClassifier(random_state=0)
gbm_clf.fit(X_train, y_train)
y_pred = gbm_clf.predict(X_test)
print(accuracy_score(y_test, y_pred) )

# 종료 시간
time.time() - start_time

# 하이퍼파라미터 튜닝
'''
  - learning_rate : 학습 시 적용되는 학습비율 -> 0.1 기본값, 이 비율을 조정해보면서 정확도를 판단
                    -> 가중치를 조절하는데 미세조정값이 이 파라미터이다 -> 미세조정값의 역할
  - n_estimators : 분류기에 사용하는 개수 -> 결정트리 개수
  - subsample : 분류기에서 사용하는 학습데이터 샘플량 비율: 현재 1.0 기본값
  - loss      : 손실함수, 경사하강법은 미세조정를 통해서 오차값을 줄이는데 목적 > 판단하는 잣대가 여러가지 함수가 존재            
'''
param_grid = {
    'learning_rate' : [0.05, 0.1, 0.01],
    'n_estimators'  : [100, 500]
}

gbm_clf = GradientBoostingClassifier( random_state=0 )
grid_cv = GridSearchCV(gbm_clf, param_grid, cv=3, verbose=True)
grid_cv.fit( X_train, y_train )

grid_cv.best_params_, grid_cv.best_score_

# 테스트 데이터를 사용한 결과
y_pred = grid_cv.best_estimator_.predict(X_test)
print(accuracy_score(y_test, y_pred) )

"""### XGBoost

- 부스팅 기법 중 가장 많이 사용됨
- GBM의 느린 속도, 과적합 규제하는 방안 없는 문제 => 개선
- 기술적으로는 병렬학습이 가능하다 => 학습효율 극대화
- 하이퍼파라미터는 GBM과 동일하고, 과적합 관련 사항 추가
"""

Image('/content/drive/MyDrive/today/분류_앙상블3_xgboost1.jpg')

Image('/content/drive/MyDrive/ComputerProgramming/today/분류_앙상블3_xgboost2.jpg')

'''
- 학습시 학습 데이터의 포맷이 별도로 존재, DMetrix 형식(Wraping)
- cv : 검증 폴드에 대한 셋업
- strified: 충화 표본 추출 
            -> 모집단(훈련 전체 데이터)들이 cv로 세트 구성할 때 
            서로 중복되지 않게 층으로 나누어서 각 층에서 데이터 추출하는 방식

- early_stopping_rounds : 조기 학습 종료 -> 어느정도 정확도, 손실값 수준으로 수렴해서, 
                          더이상의 학습이 무의미해지면 학습을 조기에 종료한다            
'''
Image('/content/drive/MyDrive/ComputerProgramming/today/분류_앙상블3_xgboost3.jpg')

# 순수 xgboost 스타일
import xgboost as xgb

# 사이킷런 스타일로 Wrapping된 클래스
# 생성 -> fit() -> predict() -> 평가도구를 활용하여 판단
# sklearn스타일로 wraping된 클래스
from xgboost import XGBClassifier

xgb.__version__

# 데이터
data_df.shape, X_train.shape, X_test.shape

data_df.head(1)

"""#### 데이터 준비(DMetrix)"""

dtrain = xgb.DMatrix(data = X_train, label = y_train) 
dtest  = xgb.DMatrix(data = X_train, label = y_train)

"""#### XGBoost 주요 하이퍼파라미터

- eta
  - 학습비용, 가중치를 조정하는 비율값
  - 사용 : 0.01 ~ 0.03 혹은 기본값
- max_depth
  - 트리 최대 깊이
  - 3 ~ 15 : 홀수값으로 지정
- min_child_weight
  - 하위 노드(바닥쪽에 존재하는 노드들) 이들이 가지고 있는 가중치의 총합
  - 최소 가중치의 합
  - 과소적합을 조정하기 위한 파라미터
  - 각 노드단계에서 발생된 가중치(w)의 총합의 제한값
  - 홀수값으로 지정
- gamma
  - 노드가 분할할 때(브랜치, 서브트리), 최소 감소값을 지정
  - 보수적으로 처리, 손실함수값(오차값)을 고려하여 조정
  - 규칙노드가 브랜치 할 때 판단되는 고려할 값
  - 0.05 ~
- subsample
  - 각 트리에서 관측되는 데이터의 샘플링 비율
  - 과소적합, 과대적합 조정 파라미터
  - 0.6 ~ 1.0 비율 지정
- colsample_bytree
  - 각 트리에서 피쳐(특성, 컬럼)샘플링 비율
  - 0.6 ~ 1.0 비율 지정
- lambda
  - 가중치(w)에 대한 L2 정규화
    - 릿지 알고리즘
- alpha
  - 가중치(w)에 대한 L1 정규화
    - 라쏘 알고리즘

- 도구 관련 파라미터
- eval_metric
  - 평가도구
  - **rmse**(root mean square error)
    - 정답과 예측 사이의 오차율 -> 작을수록 정확도가 높다
  - **mse**(mean square error)
    - 평균 제곱근 오차
  - **mae**(mean absolute error)
    - 평균 절대값 오차
  - 분류쪽에 활용
    - **logloss** : 손실에 로그처리
    - error
    - merror
    - mlogloss
    - **auc**
- seed 
  - 난수 시드
  - 재현성이 목표, 실험을 통제하기 위한 수단, 샘플 데이터를 고정하기 위한 수단
- objective
  - reg.linear
    - 기본값
  - binary : logistic
    - 이진 분류
  - multi : softmax
    - 다중 클래스 분류
    - 지분율
  - multi : softprob
    - 다중 클래스 분류
    - 지분 확률
"""

# 훈련 작성시 파라미터를 부여하여 처리할 수 있다(알고리즘 생성시에도 세팅 가능함)
params = {
    'objective' : 'binary:logistic',  # 예측된 결과는 2종류이기 때문에, 바이너리로 개별 클래스에 대한 확률값
    'eval_metric' : 'logloss',          # 학습을 언제 마무리할 지 그 평가 잣대, 손실함수를 기준으로 판단 
    'max_depth' : 3,                    # 트리의 깊이 -> 과적합 방지에 대한 옵션
    'eta' : 0.1,                        # 가중치를 조절하는 기준값, 이값을 기준으로 미세조정하여 가중치 조절
    'early_stopping_rounds' : 100       # 조기학습종료, 100회를 도달하기전에, 
                                        # 원하는 수준에 도달하고, 더이상 값의 변화가 없다면 조기 학습 종료 
                                        # 해당 조건을 만족못하면 pass 
}

evals    = [(dtrain, 'train'), (dtest, 'eval')]
'''
  로그
  [라운드] 검증시-logloss 테스트시-logloss
  [0]	train-logloss:0.60924	eval-logloss:0.615431
'''
xgbModel = xgb.train(params = params, 
           dtrain = dtrain,  
           num_boost_round = 400,
           evals = evals
          )
# 손실값이 점점 0에 수렴했고, 300회 넘어가서부터는 값의 변동폭이 거의 없었다

"""- 해석
  - 총 400회 라운드 진행(학습진행)
  - 손실값이 줄어드는 결과를 보면 -> 가중치를 조정해서 정확도를 올리는 시도를 계속 진행했다
  - 100회에서 조기학습 종료가 진행되지 않았다 -> 이 시점에서는 만족할 만한 성과가 없었다
"""

y_test

y_pred = xgbModel.predict(dtest)
# 확률값으로 나온다 -> 0과 1로 변환해야 한다
y_pred[:10], type(y_pred)

# 9.9983525e-01, 9.9729544e-01, ...,  => [ 0, 1, .... ]
'''
def trans(x) :
  if x > 0.5 :
    return 1
  else :
    return 0
tmp.apply(trans)

for n in y_pred :
  if n > 0.5:
    print(1)
  else :
    print(0)
  print(n)
  break
  상향 연산자 대체 표현

  참일 때 값 if 조건식 else 거짓일 때 값
  -> 타언어 -> 조건 ? 참일 때 값: 거짓일 때 값
'''
y_preds = [ 1 if n > 0.5 else 0 for n in y_pred ]
print( y_preds )

y_test.shape, len(y_preds)

# 성능평가, 혼동행렬 지표를 다 출력
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
def display_all_score( y_label, y_pred ):
  '''
    y_label : 정답  
    y_pred : 예측값  
  '''
  print( '혼동행렬', confusion_matrix(y_label, y_pred) )
  print( '정확도',   accuracy_score(y_label, y_pred) )
  print( '정밀도',   precision_score(y_label, y_pred) )
  print( '재현율',   recall_score(y_label, y_pred) )
  print( 'F1-score', f1_score(y_label, y_pred) )
  print( 'roc,auc',  roc_auc_score(y_label, y_pred) )
  print( '-'*30 )
  pass

display_all_score( y_test, y_pred )

# 데이터의 피처들의 중요도 시각화 도구
from xgboost import plot_importance
_, ax = plt.subplots( figsize=(16, 10) )
plot_importance(  xgbModel, ax=ax )
# 피쳐이름을 f+ 순서 형태로 제공
# 여러 모델을 이용하여 교차로 체크 후
# 영향력 업는 피쳐를 제거 후 학습하는 것도 전략

# 사이킷런 스타일로 진행
xgb_w = XGBClassifier(n_estimators = 400, random_state = 0)

eval_set = [(X_test, y_test)]
xgb_w.fit(X_train, y_train, early_stopping_rounds = 300, eval_set = eval_set,
          eval_metric = 'logloss')

xgb_w_pred = xgb_w.predict(X_test)
xgb_w_pred
# 래핑스타일이므로, 정답데이터 형태로 변환되서 나온다

display_all_score(y_test, xgb_w_pred)

"""### LightGBM

- XGBoost는 GBM 대비 속도는 개선, 다 알고리즘 보다는 느리다
- 하이퍼파라미터가 너무 많고 복잡하다
- 개선점 : 학습 속도 상승, 가볍다(리소스를 적게 사용), 파라미터의 간결화(심플)
- 특징
  - **리프를 중심으로 한 트리 분할**
  - 균형을 중시하는 방식을 비대칭이라도 리프 중심으로 옮기는데 주안점
"""

Image('/content/drive/MyDrive/today/분류_앙상블3_lightGBM.jpg')

Image('/content/drive/MyDrive/ComputerProgramming/today/분류_앙상블3_lightGBM_param.jpg')

# 파이썬 스타일 -> 자체 고유 스타일
# lightgbm 라이브러리 불러오기
import lightgbm
# sklearn 스타일
from lightgbm import LGBMClassifier

lightgbm.__version__

lgbm_clf = LGBMClassifier(n_estimators = 400)

eval_set = [(X_test, y_test)]
lgbm_clf.fit(X_train, y_train,
             early_stopping_rounds = 300,
             eval_metric = 'logloss',
             eval_set = eval_set)
# 손실값이 xgb보다 0.01dl 단축되었다
# valid_0's binary_logloss: 0.0359908	valid_0's binary_logloss: 0.0359908

# 예측 및 성능평가
y_pred = lgbm_clf.predict(X_test)
y_pred

display_all_score(y_test, y_pred)

"""- 결론
  - 부스팅 기법에서 XGBoost, LightGBM은 같이 활요하는게 적절해 보인다(성능이 거의 유사하게 나옴 -> 데이터 증설, 하이퍼파라미터 튜닝등 더 진행해 보는게 좋을듯)

## 스태킹

- stacking
- 원리
  - 여러 모델의 예측결과를 기반으로, 이 정보를 메타정보로 구축, 이를 이용해서 다시 학습 후 예측 수행
  - 모델은 2단계에 걸쳐서 생성
  - 예측결과가 데이터가 되는 구조
- 정리
  - 1차 모델: 원본 데이터를 이용하여, **여러개의 알고리즘을 활용(서로다른)** 
  - 2차 모델 : 예측 결과를 데이터로 활용하여서 학습후 예측
"""

Image('/content/drive/MyDrive/ComputerProgramming/today/분류_앙상블3_스테킹1.jpg')

Image('/content/drive/MyDrive/ComputerProgramming/today/분류_앙상블3_스테킹2.jpg')

"""### step1(1차 모델 생성)"""

# 그림에 맞춰서 코드를 전개
from sklearn.ensemble import AdaBoostClassifier

# 모델 1에서 사용할 서로 다른 여러개의 알고리즘 가져오기
knn_clf = KNeighborsClassifier(n_neighbors = 4)
rf_clf = RandomForestClassifier(n_estimators = 100)
dt_clf = DecisionTreeClassifier()
ab_clf = AdaBoostClassifier(n_estimators = 100)

"""### step2(2차 모델 생성)"""

# 순서는 경우에 따라 사용하기 전에 조정 가능
# 2차 모델 => 회귀 계열이 적절해 보임
lr_clf = LogisticRegression()

"""### step3(데이터 준비)"""

X_train.shape, X_test.shape, y_train.shape, y_test.shape

y_train[:10]

"""### step4(학습, 싸이킷런 스타일로 진행)"""

knn_clf.fit( X_train, y_train )
rf_clf.fit ( X_train, y_train )
dt_clf.fit ( X_train, y_train )
ab_clf.fit ( X_train, y_train )

"""### step5(예측)"""

knn_pred = knn_clf.predict( X_test )
rf_pred  = rf_clf.predict ( X_test )
dt_pred  = dt_clf.predict ( X_test )
ab_pred  = ab_clf.predict ( X_test )

knn_pred.shape, knn_pred[:10]

"""### step6(예측 결과를 모아서 2차 데이터 준비)"""

# 최종결과물 => (143, 4)
meta_data = np.array( [ knn_pred, rf_pred, dt_pred, ab_pred ] ).T
meta_data.shape

"""### step7(2차 모델 학습)"""

lr_clf.fit(meta_data, y_test)

"""### step8(2차 모델 예측)"""

# 정확도를 측정하기 위해 한번도 접해보지 않은 데이터를 넣어서 예측해야 한다
# 현재, 준비된 모든 데이터를 소모하였다
# 여기서는 설정한 재사용하겠다
accuracy_score(y_test, lr_clf.predict(meta_data))

# cv를 적용한 학습 과정 형태

Image('/content/drive/MyDrive/ComputerProgramming/today/분류_앙상블3_스테킹3.jpg')

Image('/content/drive/MyDrive/today/분류_앙상블3_스테킹4.jpg')

Image('/content/drive/MyDrive/ComputerProgramming/today/분류_앙상블3_스테킹5.jpg')

Image('/content/drive/MyDrive/today/분류_앙상블3_스테킹6.jpg')

"""- 위의 그림에 따라, 검증 폴드를 이용한 예측 결과를 2차 모델의 학습용 데이터로 사용
- 1차모델의 성능을 체크하기 위해 테스트 데이터로 뎨측한 결과는 모아서 2차 모델의 최종 테스트 데이터로 사용
- 직접 구현, **KFold**를 사용
"""

knn_clf = KNeighborsClassifier(n_neighbors = 4)
rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 0)
dt_clf = DecisionTreeClassifier()
ab_clf = AdaBoostClassifier(n_estimators = 100)

Image('/content/drive/MyDrive/ComputerProgramming/res/0404_res/머신러닝_학습시_검증폴드지정.png')

def make_stacking_data(clf, X_train, y_train, X_test, n_folds = 7) :
  '''
    지도학습 > 분류 > 양상블 > 스태킹> 1차메타데이터 생성 함수
    Parameter
    - clf      : 분류기
    - X_train  : 초기 훈련데이터
    - X_test   : 초기 테스트데이터
    - n_folds  : cv값, 7세트로 구성
    Returns
    - train_fold_valid_pred : 모델 학습시 내부적으로 검증폴드를 이용해 성능측정용으로 예측을 수행한다
    - test_pred_avg         : 모델 학습이 끝나면 한번도 접하지 않은 데이터(테스트)를 넣어서 
                              예측한 결과의 평균 낸 데이터(경우에 따라 평균작업은 생략가능)
    ex) 데이터 10개, cv = 5 1세트의 데이터 수는? 2개
        학습 => 4세트로 학습, 1세트로 검증 => 결과 2개                            
  '''
  # 초기 예측결과를 담는 그릇은 모두 0으로 초기화 해서 준비
  # cv를 이용한 학습 특성 상 1개의 데이터는 1번만 검증되며 예측값이 생성된다
  train_fold_valid_pred = np.zeros((X_train.shape[0], 1))
  # cv를 통해서 만들어진 세트 수만큼 테스트 데이터르 사용하여서 예측되므로 
  # 테스트 예측을 수행하는 횟수는 n_folds와 동일하다
  test_pred_avg = np.zeros((X_test.shape[0],n_folds))
  return train_fold_valid_pred, test_pred_avg

# make_stacking_data(알고리즘, 훈련용데이터, 훈련용정답데이터, 테스트용데이터)
# 리턴값
# - 검증시, 예측의 결과물을 담은 데이터 -> 2차모델의 훈련용,
# - 테스트용데이터를 넣어서 예측한 데이터 -> 2차모델의 테스트용
knn_train, knn_test = make_stacking_data(knn_clf, X_train, y_train, X_test)

# 주석 제거

from sklearn.model_selection import KFold

def make_stacking_data(clf, X_train, y_train, X_test, n_folds = 7) :
  # cv를 처리하기 위해 폴드 객체 생성
  # suffle = False => fold셋을 구성할 때 데이터를 섞지 않겠다
  kf = KFold(n_splits = n_folds)
  # [0.] ......(426, 1)
  train_fold_valid_pred = np.zeros((X_train.shape[0], 1))
  #[0. 0. 0. ... 0. 0. 0.],(143, 7)
  test_pred_avg = np.zeros((X_test.shape[0],n_folds))
  #------------------------------------------------------------
  for idx, (train_idx, valid_idx) in enumerate(kf.split(X_train)) :
    # kf_split(주어진 데이터)를 
    # 7번 반복하는 동안 훈련에 참가할 데이터의 인덱스,
    # 검증에 참가할 데이터의 인덱스를 뽑아 리턴해 준다
    # 모든 데이터는 7번 반복하는 동안 1번은 검증용으로 사용된다
    print(idx, train_idx.size, valid_idx.size)
    break
  return train_fold_valid_pred, test_pred_avg
knn_train, knn_test = make_stacking_data(knn_clf, X_train, y_train, X_test)

# 주석제거

type(X_train), type(y_train)

def make_stacking_data(clf, X_train, y_train, X_test, n_folds = 7) :
  kf = KFold(n_splits = n_folds)
  train_fold_valid_pred = np.zeros((X_train.shape[0], 1))
  test_pred_avg = np.zeros((X_test.shape[0],n_folds))
  for idx, (train_idx, valid_idx) in enumerate(kf.split(X_train)) :
    # 1. fold 기준으로 훈련데이터, 검증용데이터, 훈련용정답데이터 추출
    X_tre = X_train[train_idx] # 주어진 인덱스에 해당되는 훈련 데이터만 추출 -> 365개
    y_tre = y_train[train_idx] # 주어진 인덱스에 해당되는 정답 데이터만 추출 -> 365개
    X_val = X_train[valid_idx] # 주어진 인덱스에 해당되는 검증 데이터만 추출 -> 61개
    # 2. 훈련
    clf.fit(X_tre, y_tre)
    # 3. 예측 => 검증용 데이터로
    tmp = clf.predict(X_val)
    # 4. 검증용 데이터로 예측된 데이터를 적재
    # 2차원 데이터를 2차원 위치에 대입
    train_fold_valid_pred[valid_idx,: ] = tmp.reshape(-1, 1)
    # 5. 테스트용 데이터로 예측된 데이터를 적재
    # 1차원 데이터를 1차원 위치에 대입
    test_pred_avg[:, idx] = clf.predict(X_test)
    break
  return train_fold_valid_pred, test_pred_avg

knn_train, knn_test = make_stacking_data(knn_clf, X_train, y_train, X_test)

knn_train.shape, knn_test.shape

# 예제

tmp = np.zeros((6,1))
tmp,tmp.shape

# [1,3,5] => 인덱스 번호가 1,3,5 인 데이터를 추출하시오
tmp[[1,3,5],:]

# -1 은 해당차원은 계산하지 않겠다(자동으로 처리됨)
value = np.array([100,200,300]).reshape(-1, 1)
value

# 반복적으로 작업을 하다보면 tmp에 모든 방에 검증 후 예측된 결과가 세팅될 것이다
tmp[[1,3,5],:] = value
tmp

# 주석제거

def make_stacking_data(clf, X_train, y_train, X_test, n_folds=7):  
  kf                    = KFold( n_splits=n_folds, shuffle=False )
  train_fold_vaild_pred = np.zeros( ( X_train.shape[0], 1) )    
  test_pred_avg         = np.zeros( ( X_test.shape[0], n_folds) )
  for idx, (train_idx, vaild_idx) in enumerate( kf.split( X_train ) ):
    X_trs = X_train[ train_idx ]
    y_trs = y_train[ train_idx ]
    X_val = X_train[ vaild_idx ]
    clf.fit( X_trs, y_trs )
    tmp = clf.predict( X_val )    
    train_fold_vaild_pred[ vaild_idx , : ] = tmp.reshape(-1, 1)    
    test_pred_avg[ : , idx ]               = clf.predict( X_test )
  # 테스트 데이터 예측값 평균 처리
  # test_pred_avg : (143, 7) -> 평균처리(np.mean()) => (143,) -> (143,1)
  test_pred_avg = np.mean( test_pred_avg, axis=1).reshape( -1, 1 )
  return train_fold_vaild_pred, test_pred_avg

knn_train, knn_test = make_stacking_data(knn_clf, X_train, y_train, X_test)

knn_test.shape

knn_train, knn_test = make_stacking_data(knn_clf, X_train, y_train, X_test)
rf_train, rf_test   = make_stacking_data(rf_clf, X_train, y_train, X_test)
dt_train, dt_test   = make_stacking_data(dt_clf, X_train, y_train, X_test)
ab_train, ab_test   = make_stacking_data(ab_clf, X_train, y_train, X_test)

knn_train.shape, knn_test.shape, rf_train.shape, rf_test.shape

# 알고리즘 별로 생성된 데이터 통합
stacking_X_train = np.concatenate((knn_train, rf_train, dt_train, ab_train), axis=1)
stacking_X_train.shape

stacking_X_test = np.concatenate((knn_test, rf_test, dt_test, ab_test), axis=1)
stacking_X_test.shape

# 평균을 낸것이기 때문에 0과 1이 아닌 중간값들도 여러개 존재할수 있다
stacking_X_test[ :10 , : ]

# 2차 모델 학습
lr_clf = LogisticRegression()

lr_clf.fit(stacking_X_train, y_train)

# 예측 및 평가
accuracy_score(y_test, lr_clf.predict(stacking_X_test))

"""# SVM

## 개념

- 가장 넢은 여백을 가지게끔, 클래스로 구분할 수 있게 **경계선을 찾는다**
- 2개의 클래스를 확실하게 분류할 수 없다면, 가능한 최적의 **경계선**을 찾는다
- 이미지 분류에서 많이 사용(전통적)
"""

Image('/content/drive/MyDrive/ComputerProgramming/res/0404_res/ml-서포트백터머신.png')

"""# KNN

## 개념

- 클러스터링에서 많이 사용
"""

Image('/content/drive/MyDrive/ComputerProgramming/res/0404_res/ml-최근접이웃.png')